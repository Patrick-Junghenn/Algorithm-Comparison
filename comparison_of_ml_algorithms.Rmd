---
title: "A Comparison of Machine Learning Algorithms and Their Performance on Credit Default Data"

author: "Patrick Junghenn"
date: "Dec 1, 2018"
output: html_document
---

     Introduction and Stating the Question:
     
     Although sophisticated models such as Deep Neural Networks, Random Forests, and Gradient Boosted Machines are known for their ability to achieve highly accurate results, they are not always the first option when dealing with high dimensional data. Logistic regression can outperform some of the best machine learning algorithms when utilized in the right way for the right task. Logistic regression models are in the class of Generalized Linear Models (GLMs).  
    
     GLMs are a broader class of linear models. They handle logistic regression, and others depend on the distribution of the data at hand. GLMs have become a leading algorithm for decision making in the credit and insurance industries. A GLM's ability to provide transparency for high dimensional data makes it more attractive than complex models. GLMs are also less expensive than other machine learning models due to their simplicity. A Generalized Linear Model's ability to deliver high-quality results at low computational expenses is what makes it a favorite for credit modeling. To obtain a better understanding of a GLM's predictive superiority, we use home credit default data found on kaggle. We compare the AUC of a GLM with the AUCs of more advanced models. We use the open source software, h2o, through R, to create our comparison models.
   
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# General 
library(tidyverse)
library(skimr)
library(rlist)
# Preprocessing
library(recipes)
```

      EDA:
     
     We loaded the data and took an initial look. There are too many features in the dataset for a close inspection. We use the skim_to_list() to show essential characteristics of the data (i.e., missing values and brief descriptive statistics).
     
     The dataset contains 65,499 observations, 121 features, and one response variable. Many features are missing over 60% of the instances; we decide to use the mean, median and mode imputation method.

```{r}
credit <- read_csv("application.csv")
head(credit)

#quickly view
skim_to_list(credit)

#counts of missing value in each column
na_count <-sapply(credit, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count

#percentage of missing value
missing_tbl <- credit %>%
    summarize_all(.funs = ~ sum(is.na(.)) / length(.)) %>%
    gather() %>%
    arrange(desc(value)) %>%
    filter(value > 0)

#missing vals
print(head(missing_tbl))

#shape: 65499 X 122
dim(credit)
```

     The dataset contains 65,499 observations, 121 features, and one response variable. Many features are missing over 60% of the instances; we decide to use the mean, median and mode imputation method.

```{r, message=FALSE,warning = FALSE}
#response varible "TARGET"
target = 'TARGET'

#Train predictors--exclude response
x_train <- credit[ , !(names(credit) %in% target)]
y_train <- credit[target]  
```

     # Preprocessing  
     
     For the modeling process, we used h2o. Accessible through R, h2o is an open source machine learning platform. It uses R syntax and functions to control the entire modeling process. After the dataset is imputed, further formatting must be done to ensure h2o compatibility.
     A recipe function is used to bundle all of the preprocessing steps. Upon execution, all necessary operations are applied to the dataset to ensure h2o compatibility. The recipe() function defines the preprocessing steps.
    
```{r, message=FALSE,warning = FALSE}
#get the column name of character features
character_col_name <- colnames(x_train[, sapply(x_train, class) == 'character'])

#get the column name of numeric features whose unique counts less than 7
unique_val <- x_train %>%
    select_if(is.numeric) %>%
    map_df(~ unique(.) %>% length()) %>%
    gather() %>%
    arrange(value) %>%
    mutate(key = as_factor(key))
unique_val
factor_limit <- 7
num_col_name <- unique_val %>%
    filter(value < factor_limit) %>%
    arrange(desc(value)) %>%
    pull(key) %>%
    as.character()
```
     
     
```{r, message=FALSE,warning = FALSE}
# gather functions for baking
recipe_step <- recipe(~ ., data = x_train) %>%
    step_string2factor(character_col_name) %>%
    step_num2factor(num_col_name) %>%
    step_meanimpute(all_numeric()) %>%
    step_modeimpute(all_nominal()) %>%
    prep(stringsAsFactors = FALSE)
recipe_step

#cleaned data
x_train_processed <- bake(recipe_step, x_train) 

y_train_processed <- y_train %>%
    mutate(TARGET = TARGET %>% as.character() %>% as.factor())

```
    
    # Modeling Process and Evaluation  
    
     Credit analysts exhaust all available resources when it comes to decision making for risk mitigation â€” however, factors such as turnover rates and profitability force some data analysts to compromise slightly better accuracy achieved with a sophisticated algorithm for the transparency and similar accuracy obtained with a Generalized Linear Model. Complex machine learning algorithms are too exhausting for recurrent training and testing. Generalized linear models are less complicated but still achieve excellent results.  
     
     To see how well the GLM performs against other algorithms for credit default data, we created numerous machine learning modes that vary in length. We use the  AutoML function in h2o to generate the models so that we can compare their AUCs. The AutoML function finishes running in approximately 70 minutes. We use the AUC of each model for the comparison.  
     
     The GLM performed remarkably well, achieving an AUC higher than any other non-stacked model. The difference between the AUC of a stacked ensemble model and the AUC of a GLM is  .200285%. The GLM outperformed all other machine learning models, including GBMs and random forests.  
```{r, echo=TRUE}
#initialize h2o, and make train, validation, and test datasets
library(h2o)

#starts h2o using all CPUs
h2o.init(nthreads=-1)

#create an h2o dataset--binding the processed datasets
data_h2o <- as.h2o(bind_cols(y_train_processed, x_train_processed))

#split data training, validation, and testing.
splits_h2o <- h2o.splitFrame(data_h2o, ratios = c(0.7, 0.15), seed = 1234)
train_h2o <- splits_h2o[[1]]
valid_h2o <- splits_h2o[[2]]
test_h2o  <- splits_h2o[[3]]

y <- "TARGET"
x <- setdiff(names(train_h2o), y)

#AutoML from h2o that will create various ML models. 
automl_models_h2o <- h2o.automl(
    x = x ,
    y = y,
    training_frame    = train_h2o,
    validation_frame = test_h2o,
    max_models=5,
    seed=123
)
```

AutoML
```{r,message=FALSE,warning = FALSE}

automl_models_h2o
```

     Final Analysis and Conclusion:


     Using the held out test set, we computed the AUC for the other models. The AutoML function generates, cross-validated, and tests different kinds of machine learning models. The output shows that Stacked Ensemble models outperformed the other models. However, the AUC of the GLM is the largest of the non-ensemble algorithms
      
      In conclusion, we observed the effectiveness a GLM can be highly effective when used for the appropriate task. It grossly outperformed the more complex algorithms when applied to the same dataset. The interpretability of a GLM is superior to that of complex-black-box-algorithms.  
     
     A GLM is a desirable choice when features are essential due to its transparent results. Newer models, such as Extremely Randomized Trees and Deep Random Forests, performed poorly to GLMs. The Stacked Ensemble model achieved that highest, but it was only .48408% larger than the GLM's AUC.  The confusion matrix shows that the error rate is lower for individuals denied loans. The extension of a  loan to questionable applicants is more detrimental, and risky to a business than incorrectly denying acceptable applicants the credit.








